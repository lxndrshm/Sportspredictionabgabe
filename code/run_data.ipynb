{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount gdrive \n",
    "from google.colab import drive\n",
    "drive._mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the needed packages\n",
    "%pip install pathlib2 pandas numpy tensorflow tensorboard --quiet\n",
    "\n",
    "# Import packages.\n",
    "import os\n",
    "import signal\n",
    "import tempfile\n",
    "from pathlib2 import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for data files.\n",
    "DATA_FOLDER = '/content/gdrive/MyDrive/data/'\n",
    "FIVETHIRTYEIGHT_PROJECTS_FILE = os.path.join(DATA_FOLDER, 'spi_matches.csv')\n",
    "MARKET_VALUE_FILE = os.path.join(DATA_FOLDER, 'market_value_soccer_german_federal_league.txt')\n",
    "POINTS_FILE = os.path.join(DATA_FOLDER, 'points_soccer_german_federal_league.txt')\n",
    "PREPARED_DATA_FILE = os.path.join(DATA_FOLDER, 'own_data.csv')\n",
    "\n",
    "# Column names and values.\n",
    "FILTER_COLUMN = 'league'\n",
    "FILTER_VALUE = 'German Bundesliga'\n",
    "RESULT_COLUMN = 'result_team1'\n",
    "RESULT_HEADERS = ['loss_team1', 'draw', 'win_team1']\n",
    "RESULT_INFO_COLUMNS = ['season', 'date', 'team1', 'team2']\n",
    "VALUE_COLUMNS = [\n",
    "    'average_age',\n",
    "    'points_last_season',\n",
    "    'points_this_season',\n",
    "    'rank_last_season',\n",
    "    'squad',\n",
    "    'total_market_value_in_euro'\n",
    "    ]\n",
    "VALUE_HOME_DRAW = 1\n",
    "VALUE_HOME_LOST = 0\n",
    "VALUE_HOME_WON = 2\n",
    "\n",
    "# Parameters for using the neural net.\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "LOGS_PATH = '/content/logs'\n",
    "LOSS_FUNCTION = 'sparse_categorical_crossentropy'\n",
    "METRICS = 'sparse_categorical_accuracy'\n",
    "OPTIMIZER_FUNCTION = 'Adam'\n",
    "VALIDATION_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data.,\n",
    "data = pd.read_csv(PREPARED_DATA_FILE, delimiter=',', decimal='.')\n",
    "result_info = data[data[RESULT_COLUMN].isna()]\n",
    "result_info = result_info.loc[:, RESULT_INFO_COLUMNS]\n",
    "\n",
    "# Get the wanted columns and create dummy columns for the teams.\n",
    "data_cal = data.loc[:, VALUE_COLUMNS + [RESULT_COLUMN]]\n",
    "\n",
    "# Separate columns with match results from the ones that have none.\n",
    "data_prediction = data_cal[data_cal[RESULT_COLUMN].isna()]\n",
    "data_modeling = data_cal.dropna()\n",
    "\n",
    "# Split the data set.,\n",
    "split_index = int(len(data_modeling) * VALIDATION_SIZE)\n",
    "data_modeling = data_modeling.sample(frac=1)\n",
    "data_train = data_modeling[split_index:]\n",
    "data_valid = data_modeling[:split_index]\n",
    "\n",
    "par_train = data_train.loc[:, data_train.columns != RESULT_COLUMN]\n",
    "res_train = data_train.loc[:, [RESULT_COLUMN]]\n",
    "par_valid = data_valid.loc[:, data_train.columns != RESULT_COLUMN]\n",
    "res_valid = data_valid.loc[:, [RESULT_COLUMN]]\n",
    "data_prediction = data_prediction.loc[:, data_train.columns != RESULT_COLUMN]\n",
    "\n",
    "sel_train = par_train.to_numpy()\n",
    "sel_valid = par_valid.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "dim = sel_train.shape[1]\n",
    "nodes = dim*2-1\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(nodes, activation='relu', input_dim=dim),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define the optimizer function.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss=LOSS_FUNCTION, optimizer=optimizer, metrics=[METRICS])\n",
    "\n",
    "# Define callback function for writing data for tensorBoard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOGS_PATH, histogram_freq=1)\n",
    "\n",
    "# Run the model.\n",
    "history = model.fit(\n",
    "    x=sel_train,\n",
    "    y=res_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(sel_valid, res_valid),\n",
    "    callbacks=[tensorboard_callback],\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check if the order of the prediction data set is not changed by tensorflow => Otherwise the code below would merge the wrong infos to the predictions.\n",
    "# TODO: Check if the order of predictions results is correct (loss, draw, win) => Otherwise the table headers, need to be changed.\n",
    "\n",
    "# Make predictions.\n",
    "result_info = result_info.to_numpy()\n",
    "pre_data = data_prediction.to_numpy()\n",
    "\n",
    "predictions = model.predict(pre_data)\n",
    "\n",
    "# Show the predictions as table.\n",
    "result = []\n",
    "for i in range(0, len(predictions)):\n",
    "    item = list(result_info[i]) + list(predictions[i])\n",
    "    result.append(item)\n",
    "\n",
    "result = pd.DataFrame(result, columns = RESULT_INFO_COLUMNS + RESULT_HEADERS)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill the existing tensorboard process and delete the tensorflow temp folder. After this start a new tensorboard process.\n",
    "try:    \n",
    "    # Iterating through each instance of the process.\n",
    "    for line in os.popen(\"ps ax | grep \" + TENSORBOARD_PROCESS + \" | grep -v grep\"):\n",
    "        fields = line.split()\n",
    "            \n",
    "        # Extracting Process ID from the output.\n",
    "        pid = fields[0]\n",
    "            \n",
    "        # Terminating process.\n",
    "        os.kill(int(pid), signal.SIGKILL)\n",
    "\n",
    "    # Delete tensorboard temp folder.\n",
    "    tb_temp_folder = os.path.join(tempfile.gettempdir(), '.tensorboard-info')\n",
    "    os.system(\"rm -rf \"+tb_temp_folder)\n",
    "    print(\"Process Successfully terminated\") \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $LOGS_PATH --host localhost --port 6008"
   ]
  }
 ],
 "metadata": {
  "accelerator": "CPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorboard_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
