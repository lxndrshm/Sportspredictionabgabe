{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/DirkStulgies/mlwtSportsPrediction/blob/main/playground/teams_prediction_dirk.ipynb\"\n",
        " target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import packages.\n",
        "import os\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameters.\n",
        "LOGS_PATH = '../logs/teams_prediction_dirk'\n",
        "BASELINE_DATA_PATH = '../data/five_projects_soccer_match_data.csv'\n",
        "RESULT_LOSS_ACC_FILE = './teams_prediction_dirk_loss_acc_3.txt'\n",
        "TEAM_COLUMNS = ['team1', 'team2']\n",
        "RESULT_COLUMN = 'result_team1'\n",
        "SCORE_TEAM1_COLUMN = 'score1'\n",
        "SCORE_TEAM2_COLUMN = 'score2'\n",
        "NORMALIZE_PERCENTAGE_COLUMNS = ['spi1', 'spi2']\n",
        "VALUE_HOME_LOST = 0\n",
        "VALUE_HOME_DRAW = 1\n",
        "VALUE_HOME_WON = 2\n",
        "VALIDATION_SIZE = 0.2\n",
        "LOSS_FUNCTIONS = [\n",
        "    'categorical_hinge', 'hinge', 'huber',\n",
        "    'kullback_leibler_divergence', 'log_cosh',\n",
        "    'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_error',\n",
        "    'mean_squared_logarithmic_error', 'poisson', \n",
        "    'sparse_categorical_crossentropy', 'squared_hinge']\n",
        "OPTIMIZER_FUNCTIONS = ['Adadelta', 'Adagrad', 'Adam', 'Adamax', 'Ftrl', 'Nadam', 'RMSprop', 'SGD']\n",
        "METRICS = 'sparse_categorical_accuracy'\n",
        "EPOCHS = 300\n",
        "BATCH_SIZE = 32\n",
        "LEAGUE = 'German Bundesliga'\n",
        "\n",
        "LOGS_PATH = os.path.join(LOGS_PATH, '3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the model\n",
        "def defineModel(input_dim):\n",
        "    nodes = input_dim*2-1\n",
        "    return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(nodes, activation='relu', input_dim=input_dim),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(nodes, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(nodes, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.1),\n",
        "        tf.keras.layers.Dense(3, activation='softmax')\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def runModel(name, columns, x_train, y_train, x_valid, y_valid, optimizer, loss, epochs, metrics):\n",
        "    # Define callback function for writing data for tensorBoard\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(LOGS_PATH, name), histogram_freq=1)\n",
        "\n",
        "    # Compile and run the model.\n",
        "    model = defineModel(len(columns))\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[metrics]) \n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(x_valid, y_valid),\n",
        "        callbacks=[tensorboard_callback],\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # model evaluation\n",
        "    train_loss = history.history['loss'][EPOCHS-1] \n",
        "    train_acc = history.history[METRICS][EPOCHS-1]\n",
        "    test_loss = model.evaluate(x_valid, y_valid)[0]\n",
        "    test_acc = model.evaluate(x_valid, y_valid)[1]\n",
        "\n",
        "    # Save acc and loss results to file.\n",
        "    with open(RESULT_LOSS_ACC_FILE, 'a') as rfile:\n",
        "        rfile.write(name+'\\t'+str(train_loss)+'\\t'+str(train_acc)+'\\t'+str(test_loss)+'\\t'+str(test_acc)+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up tensorboard.\n",
        "%load_ext tensorboard\n",
        "logs = Path(LOGS_PATH)\n",
        "logs.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
        "print('Log path set to ' + LOGS_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the baseline data set.\n",
        "baseline_data = pd.read_csv(BASELINE_DATA_PATH, delimiter=',', decimal='.') \n",
        "if LEAGUE != '':\n",
        "    baseline_data = baseline_data[(baseline_data['league'] == LEAGUE)]\n",
        "baseline_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize columns with percentage values.\n",
        "bdm = baseline_data.copy(deep=True)\n",
        "for column in NORMALIZE_PERCENTAGE_COLUMNS:\n",
        "    bdm[column] = bdm[column].apply(lambda value: value / 100)\n",
        "    bdm = bdm.astype({column: np.float64})\n",
        "\n",
        "# Get a distinct list of all teams and create dummy columns.\n",
        "teams = bdm[TEAM_COLUMNS[0]].unique()\n",
        "bdm = pd.get_dummies(bdm, columns=TEAM_COLUMNS)\n",
        "feature_column_names = []\n",
        "\n",
        "for dummy_column in bdm.keys():\n",
        "    for team_column in TEAM_COLUMNS:\n",
        "        if team_column in dummy_column:\n",
        "            feature_column_names.append(dummy_column)\n",
        "\n",
        "# add the the result column.\n",
        "bdm[RESULT_COLUMN] = VALUE_HOME_DRAW\n",
        "bdm.loc[bdm[SCORE_TEAM1_COLUMN] < bdm[SCORE_TEAM2_COLUMN], RESULT_COLUMN] = VALUE_HOME_LOST\n",
        "bdm.loc[bdm[SCORE_TEAM1_COLUMN] > bdm[SCORE_TEAM2_COLUMN], RESULT_COLUMN] = VALUE_HOME_WON\n",
        "\n",
        "bdm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shuffle and split the data sets for training and validation.\n",
        "split_index = int(len(bdm) * VALIDATION_SIZE)\n",
        "bdm_shuffled = bdm.sample(frac=1)\n",
        "bdm_train = bdm_shuffled[split_index:]\n",
        "bdm_valid = bdm_shuffled[:split_index]\n",
        "\n",
        "print('Length training data:', len(bdm_train))\n",
        "print('Length validation data:', len(bdm_valid))\n",
        "\n",
        "# Select the columns for the four tests situations.\n",
        "train_only_teams = bdm_train.loc[:, feature_column_names]\n",
        "train_only_spi = bdm_train.loc[:, NORMALIZE_PERCENTAGE_COLUMNS]\n",
        "train_teams_and_spi = bdm_train.loc[:, feature_column_names + NORMALIZE_PERCENTAGE_COLUMNS]\n",
        "\n",
        "valid_only_teams = bdm_valid.loc[:, feature_column_names]\n",
        "valid_only_spi = bdm_valid.loc[:, NORMALIZE_PERCENTAGE_COLUMNS]\n",
        "valid_teams_and_spi = bdm_valid.loc[:, feature_column_names + NORMALIZE_PERCENTAGE_COLUMNS]\n",
        "\n",
        "train_result = bdm_train.loc[:, RESULT_COLUMN]\n",
        "valid_result = bdm_valid.loc[:, RESULT_COLUMN]\n",
        "\n",
        "print('Trainings shape', train_result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for optimizer in OPTIMIZER_FUNCTIONS:\n",
        "    for loss in LOSS_FUNCTIONS:\n",
        "        runModel('only_teams_'+optimizer+'_'+loss, feature_column_names, train_only_teams, train_result, valid_only_teams, valid_result, optimizer, loss, EPOCHS, METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for optimizer in OPTIMIZER_FUNCTIONS:\n",
        "    for loss in LOSS_FUNCTIONS:\n",
        "        runModel('only_spi_'+optimizer+'_'+loss, NORMALIZE_PERCENTAGE_COLUMNS, train_only_spi, train_result, valid_only_spi, valid_result, optimizer, loss, EPOCHS, METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for optimizer in OPTIMIZER_FUNCTIONS:\n",
        "    for loss in LOSS_FUNCTIONS:\n",
        "        runModel('teams_and_spi_'+optimizer+'_'+loss, feature_column_names + NORMALIZE_PERCENTAGE_COLUMNS, train_teams_and_spi, train_result, valid_teams_and_spi, valid_result, optimizer, loss, EPOCHS, METRICS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir $LOGS_PATH"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tensorboard_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
